{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "756dfd3e",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we'll walk through the process of building a simple chatbot using several powerful tools:\n",
    "\n",
    "- LangChain: A framework for developing applications powered by language models.\n",
    "- Amazon Bedrock: A service providing access to pre-trained large language models (LLMs).\n",
    "- DynamoDB: A fully managed NoSQL database service from AWS, used here to store chat history.\n",
    "- Streamlit: An open-source app framework used to create interactive web apps with Python.\n",
    "\n",
    "We'll integrate these components to build a conversational AI that can store and retrieve messages across sessions, ensuring continuity in conversation.\n",
    "### Requirements\n",
    "\n",
    "Before we begin, make sure you have the required Python packages installed. Run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5372d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade langchain_aws langchain_community langchain_core streamlit --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe67b22",
   "metadata": {},
   "source": [
    "## Setting Up Bedrock with LangChain\n",
    "\n",
    "The first step in building our chatbot is to connect to Amazon Bedrock. Bedrock allows us to use pre-trained language models to generate text responses. We'll be using the `ChatBedrockConverse` class from the `langchain_aws` package to interface with the Bedrock service.\n",
    "\n",
    "### Creating a Bedrock Model Instance\n",
    "\n",
    "We start by creating an instance of the `ChatBedrockConverse` class, which will allow us to send prompts to the Bedrock service and receive responses generated by the Anthropic Claude model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb894352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "model = ChatBedrockConverse(\n",
    "    model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    max_tokens=2048,\n",
    "    temperature=0.0,\n",
    "    top_p=1,\n",
    "    stop_sequences=[\"\\n\\nHuman\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da88330f",
   "metadata": {},
   "source": [
    "### Testing the Bedrock Model\n",
    "\n",
    "To understand how the model works and ensure it’s correctly set up, we’ll perform some basic tests by invoking the model with simple prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21923967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"It's nice to meet you, Bob! I'm Claude, an AI assistant created by Anthropic. I'm here to help with any questions or tasks you might have. Please let me know if there's anything I can assist you with.\" response_metadata={'ResponseMetadata': {'RequestId': '79734851-bb3c-420f-8835-b441a0417b90', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 09 Aug 2024 15:06:16 GMT', 'content-type': 'application/json', 'content-length': '387', 'connection': 'keep-alive', 'x-amzn-requestid': '79734851-bb3c-420f-8835-b441a0417b90'}, 'RetryAttempts': 0}, 'stopReason': 'stop_sequence', 'metrics': {'latencyMs': 1118}} id='run-cc12e5a1-a09b-4d80-81e2-dce56531cfe3-0' usage_metadata={'input_tokens': 12, 'output_tokens': 52, 'total_tokens': 64}\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(input=\"Hi! I'm Bob\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a674d8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I'm afraid I don't actually know your name. As an AI assistant, I don't have personal information about you unless you provide it to me directly.\" response_metadata={'ResponseMetadata': {'RequestId': 'e588c3fa-b7c2-472d-a0f0-f65411d00241', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 09 Aug 2024 15:06:19 GMT', 'content-type': 'application/json', 'content-length': '331', 'connection': 'keep-alive', 'x-amzn-requestid': 'e588c3fa-b7c2-472d-a0f0-f65411d00241'}, 'RetryAttempts': 0}, 'stopReason': 'stop_sequence', 'metrics': {'latencyMs': 688}} id='run-d65ea2c7-555b-4ac4-94a7-18bc7002b463-0' usage_metadata={'input_tokens': 12, 'output_tokens': 34, 'total_tokens': 46}\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(input=\"What's my name?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81150d5",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "#### Purpose of the Test:\n",
    "The first test (Hi! I'm Bob) introduces the user to the model. Since this is the first interaction, the model doesn't have any previous context about who \"Bob\" is.\n",
    "\n",
    "The second test (What's my name?) asks the model to recall the name from the first input. However, because there’s no conversation context provided to the model, it should fail to recognize the user's name.\n",
    "\n",
    "#### Expected Outcome:\n",
    "The model will not recognize the user's name in the second prompt because there is no chat history or context being passed along with the input. This demonstrates the need for a persistent conversation history, which we will address by integrating DynamoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271e8a76",
   "metadata": {},
   "source": [
    "## Setting Up DynamoDB for Chat History\n",
    "\n",
    "Now that we've established that the model needs context to remember past interactions, we’ll set up a DynamoDB table. This table will store the chat history, allowing our chatbot to maintain a continuous conversation over multiple interactions.\n",
    "\n",
    "### Creating the DynamoDB Table\n",
    "\n",
    "We’ll use Python boto3 client to create a table that will store each user session’s chat history. The table will use the `SessionId` as the primary key to ensure that each session’s history is stored separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664518bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "dynamodb = boto3.resource(\"dynamodb\")\n",
    "client = boto3.client('sts')\n",
    "\n",
    "try:\n",
    "    # Create the DynamoDB table.\n",
    "    table = dynamodb.create_table(\n",
    "        TableName=\"SessionTable\",\n",
    "        KeySchema=[{\"AttributeName\": \"SessionId\", \"KeyType\": \"HASH\"}],\n",
    "        AttributeDefinitions=[{\"AttributeName\": \"SessionId\", \"AttributeType\": \"S\"}],\n",
    "        BillingMode=\"PAY_PER_REQUEST\",\n",
    "    )\n",
    "    # Wait until the table exists.\n",
    "    table.meta.client.get_waiter(\"table_exists\").wait(TableName=\"SessionTable\")\n",
    "\n",
    "    # Print out some data about the table.\n",
    "    print(table.item_count)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85616183",
   "metadata": {},
   "source": [
    "## Retrieving the IAM User ID\n",
    "\n",
    "To uniquely identify each session, we’ll use the IAM User ID associated with the AWS account running this application. This ensures that each user's chat history is stored separately and securely.\n",
    "- The IAM User ID is a unique identifier for the AWS user or role under which the application is running. By using this ID as the session identifier, we ensure that each user or role has a distinct and separate conversation history.\n",
    "- This approach also enhances security and auditability, as each session is tied to a specific AWS identity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4311b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iam_user_id():    \n",
    "    # Get the caller identity\n",
    "    identity = client.get_caller_identity()\n",
    "    \n",
    "    # Extract and return the User ID\n",
    "    user_id = identity['UserId']\n",
    "    return user_id\n",
    "\n",
    "# Get UserId for sessionId\n",
    "user_id = get_iam_user_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756d827a",
   "metadata": {},
   "source": [
    "## Integrating LangChain with DynamoDB\n",
    "\n",
    "With our DynamoDB table ready, we can now integrate it with LangChain. This will enable our chatbot to store and retrieve conversation history, allowing it to maintain context across multiple interactions.\n",
    "\n",
    "### Setting Up the Chat History and Prompt System\n",
    "\n",
    "We'll use the `DynamoDBChatMessageHistory` class to interact with DynamoDB and store chat messages. The LangChain prompt system will then use this stored history to provide context to the Bedrock model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a71272c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import DynamoDBChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "# Initialize the DynamoDB chat message history\n",
    "history = DynamoDBChatMessageHistory(table_name=\"SessionTable\", session_id=\"0\")\n",
    "\n",
    "# Create the chat prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create output parser to simplify the output\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Combine the prompt with the Bedrock LLM\n",
    "chain = prompt | model| output_parser\n",
    "\n",
    "# Integrate with message history\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: DynamoDBChatMessageHistory(\n",
    "        table_name=\"SessionTable\", session_id=session_id\n",
    "    ),\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "# Invoke the chain with a session-specific configuration\n",
    "config = {\"configurable\": {\"session_id\": user_id}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8985eb39",
   "metadata": {},
   "source": [
    "### Testing the Integrated System\n",
    "\n",
    "Now, let's test the integration by sending some prompts and verifying that the model can maintain context across different interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f39b2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's nice to meet you, Bob! I'm an AI assistant created by Anthropic. I'm here to help with any questions or tasks you might have. Please let me know if there's anything I can assist you with.\n"
     ]
    }
   ],
   "source": [
    "response = chain_with_history.invoke({\"question\": \"Hi! I'm Bob\"}, config=config)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01dcd47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Bob, as you introduced yourself to me earlier.\n"
     ]
    }
   ],
   "source": [
    "response = chain_with_history.invoke({\"question\": \"What's my name?\"}, config=config)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db867a82",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "#### Purpose of the Test:\n",
    "This time, when we ask \"What's my name?\" after introducing ourselves as \"Bob\", the chatbot should remember the name because the context is stored in DynamoDB and passed along with each prompt.\n",
    "#### Expected Outcome:\n",
    "The chatbot should correctly identify the user's name as \"Bob\" in the second prompt, demonstrating that it now maintains context across interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed80da6",
   "metadata": {},
   "source": [
    "## Creating a Streamlit Interface\n",
    "\n",
    "To interact with our chatbot in a user-friendly way, we’ll build a simple web interface using Streamlit. This interface will allow us to input questions and see responses in real-time, with the context being managed by DynamoDB.\n",
    "\n",
    "### Step 1: Initial Streamlit Setup\n",
    "\n",
    "We start by creating a basic echo bot to familiarize ourselves with Streamlit. This will later be replaced by our AI-driven chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8eb61fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"Echo Bot\")\n",
    "\n",
    "# Initialize chat history\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat messages from history on app rerun\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# React to user input\n",
    "if prompt := st.chat_input(\"What is up?\"):\n",
    "    # Display user message in chat message container\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "    # Add user message to chat history\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "response = f\"Echo: {prompt}\"\n",
    "# Display assistant response in chat message container\n",
    "with st.chat_message(\"assistant\"):\n",
    "    st.markdown(response)\n",
    "# Add assistant response to chat history\n",
    "st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1809d86",
   "metadata": {},
   "source": [
    "## Running the Application\n",
    "\n",
    "Finally, you can run your Streamlit application with the following command. This will launch the Streamlit application and allow you to interact with the chatbot through a web interface.\n",
    "\n",
    "Take your notebook URL from the current web page and format it like the following in another tab in your internet browser:\n",
    "\n",
    "`https://<unique_notebook_id>.notebook.us-west-2.sagemaker.aws/proxy/absolute/8501`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70704589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501/proxy/absolute/8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.16.87.38:8501/proxy/absolute/8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://52.42.79.222:8501/proxy/absolute/8501\u001b[0m\n",
      "\u001b[0m\n",
      "^C\n",
      "\u001b[34m  Stopping...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! streamlit run /home/ec2-user/SageMaker/app.py --server.baseUrlPath=\"proxy/absolute/8501\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3fbca6",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "#### Purpose of the Echo Bot:\n",
    "\n",
    "This simple application echoes back whatever the user types. It's a basic example to show how messages can be displayed and stored in the session state.\n",
    "\n",
    "![](../../Bedrock-Examples/Langchain/img/echo.png)\n",
    "\n",
    "#### Before proceeding\n",
    "Be sure to stop the above cell. You can do this by highlighting it and pressing the stop button on the navigation pane at the top of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba86d05",
   "metadata": {},
   "source": [
    "## Step 2: Integrating Bedrock and DynamoDB with Streamlit\n",
    "\n",
    "Next, we'll replace the echo bot logic with our AI-powered chatbot that uses Bedrock for generating responses and DynamoDB for managing chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14f9f718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import boto3\n",
    "import logging\n",
    "import streamlit as st\n",
    "from langchain_community.chat_message_histories import DynamoDBChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "client = boto3.client('sts')\n",
    "logging.basicConfig(level=logging.CRITICAL)\n",
    "\n",
    "def get_iam_user_id():    \n",
    "    # Get the caller identity\n",
    "    identity = client.get_caller_identity()\n",
    "    \n",
    "    # Extract and return the User ID\n",
    "    user_id = identity['UserId']\n",
    "    return user_id\n",
    "\n",
    "model = ChatBedrockConverse(\n",
    "    model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    max_tokens=2048,\n",
    "    temperature=0.0,\n",
    "    top_p=1,\n",
    "    stop_sequences=[\"\\n\\nHuman\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Initialize the DynamoDB chat message history\n",
    "table_name = \"SessionTable\"\n",
    "session_id = get_iam_user_id()  # You can make this dynamic based on the user session\n",
    "history = DynamoDBChatMessageHistory(table_name=table_name, session_id=session_id)\n",
    "\n",
    "# Create the chat prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Combine the prompt with the Bedrock LLM\n",
    "chain = prompt_template | model | output_parser\n",
    "\n",
    "# Integrate with message history\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    lambda session_id: DynamoDBChatMessageHistory(\n",
    "        table_name=table_name, session_id=session_id\n",
    "    ),\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "st.title(\"LangChain DynamoDB Bot\")\n",
    "\n",
    "# Load messages from DynamoDB and populate chat history\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "    \n",
    "    # Load the stored messages from DynamoDB\n",
    "    stored_messages = history.messages  # Retrieve all stored messages\n",
    "    \n",
    "    # Populate the session state with the retrieved messages\n",
    "    for msg in stored_messages:\n",
    "        role = \"user\" if msg.__class__.__name__ == \"HumanMessage\" else \"assistant\"\n",
    "        st.session_state.messages.append({\"role\": role, \"content\": msg.content})\n",
    "\n",
    "# Display chat messages from history on app rerun\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# React to user input\n",
    "if prompt := st.chat_input(\"What is up?\"):\n",
    "    # Display user message in chat message container\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "    # Add user message to chat history\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    # Generate assistant response using Bedrock LLM and LangChain\n",
    "    config = {\"configurable\": {\"session_id\": session_id}}\n",
    "    response = chain_with_history.invoke({\"question\": prompt}, config=config)\n",
    "\n",
    "    # Display assistant response in chat message container\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        st.markdown(response)\n",
    "    # Add assistant response to chat history\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fefae34",
   "metadata": {},
   "source": [
    "### ## Running the Application\n",
    "\n",
    "Finally, you can run your Streamlit application with the following command. This will launch the Streamlit application and allow you to interact with the chatbot through a web interface.\n",
    "\n",
    "Take your notebook URL from the current web page and format it like the following in another tab in your internet browser:\n",
    "\n",
    "`https://<unique_notebook_id>.notebook.us-west-2.sagemaker.aws/proxy/absolute/8501`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae5d9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501/proxy/absolute/8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.16.87.38:8501/proxy/absolute/8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://52.42.79.222:8501/proxy/absolute/8501\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! streamlit run /home/ec2-user/SageMaker/app.py --server.baseUrlPath=\"proxy/absolute/8501\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8686d03",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "The Streamlit app now interacts with the Bedrock model and DynamoDB to provide responses that maintain context over time. Each session is uniquely identified by the IAM User ID, ensuring that different users or roles have separate conversation histories.\n",
    "\n",
    "#### Streamlit UI:\n",
    "The interface is designed to display past conversations and generate new responses based on user input. This makes it easy to interact with the chatbot and see the context-aware responses.\n",
    "\n",
    "![](../../Bedrock-Examples/Langchain/img/chatbot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd7bb1a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
