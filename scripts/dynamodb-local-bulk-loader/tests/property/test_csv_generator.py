#
#  Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
#  This file is licensed under the Apache License, Version 2.0 (the "License").
#  You may not use this file except in compliance with the License. A copy of
#  the License is located at
#
#  http://aws.amazon.com/apache2.0/
#
#  This file is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR
#  CONDITIONS OF ANY KIND, either express or implied. See the License for the
#  specific language governing permissions and limitations under the License.
#
"""Property-based tests for CSV generator.

This module contains property-based tests using Hypothesis to verify
universal correctness properties of the CSVGenerator class.
"""

import csv
import tempfile
import uuid
from datetime import datetime
from pathlib import Path

from hypothesis import given, settings
from hypothesis import strategies as st

from src.csv_generator import CSVGenerator


# Feature: dynamodb-csv-bulk-loader, Property 8: UUID Generation Validity
# For any record generated by the CSV generator, the ID field should be
# a valid UUID v4 format.
@given(num_records=st.integers(min_value=1, max_value=100))
@settings(max_examples=100)
def test_uuid_generation_validity(num_records: int) -> None:
    """Property test: All generated IDs must be valid UUID v4 format.

    This test verifies that every record generated by the CSV generator
    has a valid UUID v4 as its ID field, which is critical for ensuring
    unique primary keys in DynamoDB.

    Validates: Requirements 4.1
    """
    # Create temporary file for CSV output
    with tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False) as tmp_file:
        tmp_path = tmp_file.name

    try:
        # Generate CSV with specified number of records
        generator = CSVGenerator(output_file=tmp_path, num_records=num_records)
        generator.generate()

        # Read the generated CSV
        with open(tmp_path, encoding="utf-8") as csvfile:
            reader = csv.DictReader(csvfile)
            records = list(reader)

        # Verify we got the expected number of records
        assert len(records) == num_records, f"Expected {num_records} records, got {len(records)}"

        # Verify each ID is a valid UUID
        for i, record in enumerate(records):
            id_value = record["id"]

            # Attempt to parse as UUID - this will raise ValueError if invalid
            try:
                parsed_uuid = uuid.UUID(id_value)
                # Verify it's a valid UUID v4
                assert (
                    parsed_uuid.version == 4
                ), f"Record {i}: UUID must be version 4, got version {parsed_uuid.version}"
            except ValueError as e:
                raise AssertionError(f"Record {i}: Invalid UUID format '{id_value}': {e}") from e

            # Additional validation: UUID string format
            assert (
                len(id_value) == 36
            ), f"Record {i}: UUID must be 36 characters, got {len(id_value)}"
            assert (
                id_value.count("-") == 4
            ), f"Record {i}: UUID must have 4 hyphens, got {id_value.count('-')}"

    finally:
        # Clean up temporary file
        Path(tmp_path).unlink(missing_ok=True)


# Feature: dynamodb-csv-bulk-loader, Property 10: Timestamp Sorting
# For any dataset generated by the CSV generator, the timestamps should be
# in ascending chronological order.
@given(num_records=st.integers(min_value=2, max_value=100))
@settings(max_examples=100)
def test_timestamp_sorting(num_records: int) -> None:
    """Property test: Generated timestamps must be in ascending chronological order.

    This test verifies that timestamps in the generated CSV are sorted in
    ascending order, which creates the hot partition scenario that the
    shuffle pattern is designed to prevent.

    Validates: Requirements 4.4
    """
    # Create temporary file for CSV output
    with tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False) as tmp_file:
        tmp_path = tmp_file.name

    try:
        # Generate CSV with specified number of records
        generator = CSVGenerator(output_file=tmp_path, num_records=num_records)
        generator.generate()

        # Read the generated CSV
        with open(tmp_path, encoding="utf-8") as csvfile:
            reader = csv.DictReader(csvfile)
            records = list(reader)

        # Extract and parse timestamps
        timestamps = []
        for i, record in enumerate(records):
            timestamp_str = record["timestamp"]
            try:
                timestamp = datetime.fromisoformat(timestamp_str)
                timestamps.append(timestamp)
            except ValueError as e:
                raise AssertionError(
                    f"Record {i}: Invalid ISO 8601 timestamp '{timestamp_str}': {e}"
                ) from e

        # Verify timestamps are in ascending order
        for i in range(len(timestamps) - 1):
            current = timestamps[i]
            next_ts = timestamps[i + 1]
            assert (
                current <= next_ts
            ), f"Timestamps not sorted: record {i} ({current}) > record {i+1} ({next_ts})"

        # Additional check: verify timestamps are strictly increasing (not just non-decreasing)
        # This ensures we're creating a realistic hot partition scenario
        unique_timestamps = set(timestamps)
        assert len(unique_timestamps) == len(
            timestamps
        ), "Timestamps should be unique for realistic hot partition scenario"

    finally:
        # Clean up temporary file
        Path(tmp_path).unlink(missing_ok=True)


# Feature: dynamodb-csv-bulk-loader, Property 11: Record Count Accuracy
# For any configured record count N, the CSV generator should produce
# exactly N records in the output file.
@given(num_records=st.integers(min_value=0, max_value=1000))
@settings(max_examples=100, deadline=500)
def test_record_count_accuracy(num_records: int) -> None:
    """Property test: Generated CSV must contain exactly the requested number of records.

    This test verifies that the CSV generator produces exactly the number
    of records specified in the configuration, which is critical for
    testing and validation purposes.

    Validates: Requirements 4.5
    """
    # Create temporary file for CSV output
    with tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False) as tmp_file:
        tmp_path = tmp_file.name

    try:
        # Generate CSV with specified number of records
        generator = CSVGenerator(output_file=tmp_path, num_records=num_records)
        generator.generate()

        # Read the generated CSV
        with open(tmp_path, encoding="utf-8") as csvfile:
            reader = csv.DictReader(csvfile)
            records = list(reader)

        # Verify exact count
        actual_count = len(records)
        assert (
            actual_count == num_records
        ), f"Expected exactly {num_records} records, got {actual_count}"

        # If records were generated, verify they have all required fields
        if num_records > 0:
            required_fields = [
                "id",
                "timestamp",
                "category",
                "user_name",
                "email",
                "amount",
                "status",
                "description",
            ]
            for field in required_fields:
                assert field in records[0], f"Required field '{field}' missing from CSV header"

    finally:
        # Clean up temporary file
        Path(tmp_path).unlink(missing_ok=True)


# Feature: dynamodb-csv-bulk-loader, Property 12: CSV Format Round Trip
# For any CSV file generated by the CSV generator, parsing the file and
# then regenerating it should produce an equivalent dataset with the same
# records and structure.
@given(num_records=st.integers(min_value=1, max_value=50))
@settings(max_examples=100)
def test_csv_format_round_trip(num_records: int) -> None:
    """Property test: CSV format must support round-trip read/write operations.

    This test verifies that a CSV file can be read and its data structure
    preserved, which is essential for the loader implementations to correctly
    parse and process the generated data.

    Validates: Requirements 4.8
    """
    # Create temporary files for original and round-trip CSV
    with tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False) as tmp_file1:
        tmp_path1 = tmp_file1.name
    with tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False) as tmp_file2:
        tmp_path2 = tmp_file2.name

    try:
        # Generate original CSV
        generator = CSVGenerator(output_file=tmp_path1, num_records=num_records)
        generator.generate()

        # Read the original CSV
        with open(tmp_path1, encoding="utf-8") as csvfile:
            reader = csv.DictReader(csvfile)
            original_records = list(reader)
            fieldnames = reader.fieldnames

        # Ensure fieldnames is not None for type safety
        assert fieldnames is not None, "CSV must have fieldnames"

        # Write the records to a new CSV (round trip)
        with open(tmp_path2, "w", newline="", encoding="utf-8") as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(original_records)

        # Read the round-trip CSV
        with open(tmp_path2, encoding="utf-8") as csvfile:
            reader = csv.DictReader(csvfile)
            roundtrip_records = list(reader)

        # Verify same number of records
        assert len(original_records) == len(
            roundtrip_records
        ), f"Record count mismatch: {len(original_records)} vs {len(roundtrip_records)}"

        # Verify each record matches
        for i, (original, roundtrip) in enumerate(zip(original_records, roundtrip_records)):
            # Check all fields are present
            assert original.keys() == roundtrip.keys(), f"Record {i}: Field names don't match"

            # Check all values match
            for field in original.keys():
                original_value = original[field]
                roundtrip_value = roundtrip[field]
                assert (
                    original_value == roundtrip_value
                ), f"Record {i}, field '{field}': '{original_value}' != '{roundtrip_value}'"

        # Verify field order is preserved
        assert (
            fieldnames == reader.fieldnames
        ), f"Field order not preserved: {fieldnames} vs {reader.fieldnames}"

    finally:
        # Clean up temporary files
        Path(tmp_path1).unlink(missing_ok=True)
        Path(tmp_path2).unlink(missing_ok=True)
